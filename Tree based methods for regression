Petrol Consumption
------------------
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv("/content/petrol_consumption.csv")
df.head()

df.info()
df.describe()

# Remove leading and trailing whitespace from column names
df.columns = df.columns.str.strip()
df.columns

x=df.drop("Petrol_Consumption",axis=1)
y=df["Petrol_Consumption"]

from sklearn.model_selection import train_test_split as tts
x_train,x_test,y_train,y_test=tts(x,y,test_size=0.2,random_state=0)
x_train.shape,y_train.shape

from sklearn.tree import DecisionTreeRegressor
dt=DecisionTreeRegressor()
dt.fit(x_train,y_train)
y_pred=dt.predict(x_test)
y_pred

df1=pd.DataFrame({"Actual":y_test,"Predicted":y_pred})
df1

# Measure the performance of the model:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt

mae=mean_absolute_error(y_test,y_pred)
mse=mean_squared_error(y_test,y_pred)
rmse=np.sqrt(mean_squared_error(y_test,y_pred))

mae,mse,rmse
_________________________________________________________________________
Ensemble Method: Random Forest

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv("/content/petrol_consumption.csv")
df.head()

# Remove leading and trailing whitespace from column names
df.columns = df.columns.str.strip()
df.columns

x=df.drop("Petrol_Consumption",axis=1)
y=df["Petrol_Consumption"]

from sklearn.model_selection import train_test_split as tts
x_train,x_test,y_train,y_test=tts(x,y,test_size=0.2,random_state=0)
x_train.shape,y_train.shape

# Importing RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor(n_estimators=200,bootstrap=True,max_features="sqrt")
rf.fit(x_train,y_train)
y_pred=rf.predict(x_test)
y_pred

df2=pd.DataFrame({"Actual":y_test,"Predicted":y_pred})
df2

# Measure the performance of the model:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt
from sklearn import metrics

mae=metrics.mean_absolute_error(y_test,y_pred)
mse=metrics.mean_squared_error(y_test,y_pred)
rmse=np.sqrt(metrics.mean_squared_error(y_test,y_pred))
mae,mse,rmse

feature_list=x_train.columns
importances=list(rf.feature_importances_)

rf_imp=RandomForestRegressor(n_estimators=500,random_state=5)
imp_indices=[feature_list[2],feature_list[1]]
train_imp=x_train.loc[:,['Average_income', 'Paved_Highways','Population_Driver_licence(%)']]
test_imp=x_test.loc[:,['Average_income', 'Paved_Highways','Population_Driver_licence(%)']]
rf_imp.fit(train_imp,y_train)
y_predictions=rf_imp.predict(test_imp)
y_predictions

# Measure the performance of the model:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt

mae=mean_absolute_error(y_test,y_predictions)
mse=mean_squared_error(y_test,y_predictions)
rmse=np.sqrt(mean_squared_error(y_test,y_predictions))
mae,mse,rmse
___________________________________________________________________
Feature_Selection Using Tree-Based Methods

# Feature_Selection using tree-based methods:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

df=pd.read_csv("/content/House_data.csv")
df.head()

# perform data preprocessing 
df["basement"]=(df["sqft_basement"]>0).astype(int)
df["renovated"]=(df["yr_renovated"]>0).astype(int)
to_drop=["sqft_basement","yr_renovated"]
to_drop

df=df.drop(to_drop,axis=1,inplace=True)
cat_var=["waterfront",'floors',"view","condition","grade"]
df1=pd.get_dummies(df,columns=cat_var,drop_first=True)
df1

x=df.iloc[:,1:].values
y=df.iloc[:,0].values

3 Now lets create a extratreesclassifier
from sklearn.ensemble import ExtraTreesClassifier
tree_cl=ExtraTreesClassifier()
tree_cl.fit(x,y)

# we are now getting the respective impts of various variables and ordering them in desecnding order o impts
importants=tree_cl.feature_importance
feature_name=df.iloc[:,1:].columns.tolist()
feature_names
feature_imp=dict(zip(feature_names,importance))
features=sorted(feature_imp,importance.items(),key=lambda x:x[1],reverse=True)
feature_imp

# Now we will visualize the features in the order of their impts
plt.bar(range(len(features)),[imp[1] for imp in features],align="center")
plt.title("The important feature in House Data")

# we can now analyze how many variables have been selected and how many of them have been removed
from sklearn.feature_selection import SelectFromModel
abc=SelectFromModel(tree_cl,prefit=True)
x_updated=abc.transform(x)
print("total feature count":np.array(x).shape[1])
print("selected feature":np.array(x_updated).shape[1])
__________________________________________________________________________
